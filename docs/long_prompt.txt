The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.

The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true. Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project.

In the 1970s, criticism from James Lighthill and ongoing pressure from the US Congress to fund more productive projects led the US and British governments to cut off exploratory research in AI. The next few years would later be called an "AI winter", a period when obtaining funding for AI projects was difficult. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts.

By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the US and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition.

A number of researchers began to look into "sub-symbolic" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Neural networks research had been abandoned by AI and computer science around the same time. This line of research was revived in the middle 1980s by David Rumelhart and others. These developments and their application to related problems such as speech recognition and computer vision gave rise to what has since been called "the AI spring" of the 1990s.

Deep learning began to dominate industry benchmarks in 2012 and was adopted by the AI community. For many specific tasks, other methods were abandoned. Deep learning uses many-layered neural networks, inspired by the structure of the neurons in human brains. This technique is especially effective for learning sequential patterns. By 1999, the majority of AI researchers had switched to related fields such as machine learning, robotics, and machine perception. The field was reunited under the new paradigm of statistical learning theory.

The term "artificial general intelligence" (AGI) was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. The term "strong AI" was originally coined by John Searle in 1980 to differentiate AI systems that can reason and have consciousness from those that cannot.

Transformers were introduced in 2017. This was a significant development in AI because transformers could be parallelized, making them more efficient to train than previous sequence models. In 2018, GPT was released by OpenAI, followed in 2019 by GPT-2. ChatGPT, released in November 2022, became increasingly popular, reaching over 100 million users in two months. Microsoft's investment in OpenAI and integration of GPT-4 into Bing and other products further propelled AI into mainstream consciousness.

Large language models (LLMs) are a type of artificial neural network designed to understand and generate human language. LLMs are built using a specific architecture known as a transformer. The transformer architecture was developed by researchers at Google and was introduced in the seminal paper "Attention Is All You Need" in 2017. The key innovation of the transformer is the self-attention mechanism which allows the model to weigh the importance of different parts of the input when generating output.

The attention mechanism works by computing three vectors for each input token: a query vector, a key vector, and a value vector. The attention scores are computed by taking the dot product of the query vector with all key vectors, scaling the result, and applying a softmax function. These scores are then used to compute a weighted sum of the value vectors. This process allows the model to focus on different parts of the input sequence when generating each output token.

Modern LLMs contain billions of parameters and are trained on massive datasets. Training involves predicting the next token in a sequence given the previous tokens. This simple objective, when applied at scale, leads to models that can perform a wide variety of language tasks. The training process uses gradient descent to update the model parameters, minimizing the difference between the model's predictions and the actual next tokens in the training data.

Inference in LLMs involves generating text one token at a time. Given a prompt, the model computes the probability distribution over the next token, samples from this distribution, and appends the sampled token to the input. This process is repeated until a stopping condition is met, such as generating a special end-of-sequence token or reaching a maximum length. The autoregressive nature of this process means that inference can be slow for long sequences.

Various optimization techniques have been developed to speed up both training and inference. These include mixed-precision training, which uses lower-precision floating-point numbers to reduce memory usage and increase throughput; gradient checkpointing, which trades compute for memory by recomputing intermediate activations during the backward pass; and speculative decoding, which uses a smaller draft model to generate candidate tokens that are then verified by the larger model.

The key-value (KV) cache is an important optimization for inference. During autoregressive generation, the key and value vectors for each token remain constant once computed. By caching these vectors, subsequent tokens only need to compute new key and value vectors for themselves, rather than recomputing for the entire sequence. This reduces the computational complexity of each step from O(n^2) to O(n), where n is the sequence length.

Sparse attention is another important optimization that reduces the complexity of the attention mechanism. Instead of computing attention over all tokens in the sequence, sparse attention only computes attention over a subset of tokens. Various sparsity patterns have been proposed, including local attention (attending only to nearby tokens), strided attention (attending to every k-th token), and learned sparsity patterns. These techniques can reduce the complexity of attention from O(n^2) to O(n) or even O(n log n).

Quantization is a technique that reduces the precision of the model weights and activations to lower bit-widths. Common approaches include 8-bit integer quantization (INT8) and 4-bit quantization. Quantization can significantly reduce memory usage and increase inference speed, with minimal impact on model quality when done carefully. Post-training quantization applies quantization after the model has been trained, while quantization-aware training incorporates quantization during the training process.

The mixture of experts (MoE) architecture is an approach to scaling LLMs that increases model capacity without proportionally increasing computational cost. In an MoE model, each layer contains multiple "expert" networks, but only a subset of these experts are activated for each input. A gating network learns to route inputs to the most appropriate experts. This sparse activation pattern allows MoE models to have more parameters than dense models while using similar computational resources during inference.

Retrieval-augmented generation (RAG) is a technique that combines LLMs with external knowledge bases. Instead of relying solely on the knowledge encoded in the model's parameters, RAG systems retrieve relevant documents from a knowledge base and include them in the prompt. This approach allows the model to access up-to-date information and reduces hallucination by grounding the model's responses in retrieved facts.

Fine-tuning is the process of adapting a pre-trained LLM to a specific task or domain. This typically involves training the model on a smaller, task-specific dataset. Parameter-efficient fine-tuning methods, such as LoRA (Low-Rank Adaptation), update only a small number of additional parameters while keeping the original model weights frozen. This reduces memory requirements and allows multiple fine-tuned models to share the same base model.

Reinforcement learning from human feedback (RLHF) is a technique used to align LLMs with human preferences. The process involves three steps: first, a reward model is trained to predict human preferences between different model outputs; second, the LLM is fine-tuned using reinforcement learning to maximize the reward model's scores; finally, the process may be repeated to iteratively improve alignment. RLHF has been instrumental in making models like ChatGPT more helpful, harmless, and honest.

The scaling laws of LLMs describe how model performance improves as various factors are scaled up. Research has shown that performance improves predictably as the model size, dataset size, and compute budget are increased. These findings have led to a race to train ever-larger models, with the latest models containing hundreds of billions or even trillions of parameters. However, there are ongoing debates about whether this scaling approach will lead to artificial general intelligence or whether fundamentally new approaches are needed.
